import pandas as pd
from sklearn.metrics import r2_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, balanced_accuracy_score
import numpy as np

from clusterize_by_leafs import LeafClusterThresholdOptimizer
from clusterize_by_k_means import FeatureClusterThresholdOptimizer, MixedDataClusterOptimizer


def test_with_clusters_weighted_mixed(_model, _test_data, _cat_features,
                                         feature_optimizer=None,
                                         comparison_metric='balanced_accuracy',
                                         _inference=False):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ (MixedDataClusterOptimizer).

    :param _model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å CatBoost
    :param _test_data: —Å–ø–∏—Å–æ–∫ DataFrame —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    :param _cat_features: —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :param feature_optimizer: –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π MixedDataClusterOptimizer
    :param comparison_metric: –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è ('balanced_accuracy', 'f1')
    :param _inference: —Ñ–ª–∞–≥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    """

    pos_class = 700
    neg_class = 5500

    test_data_all = pd.concat(_test_data, axis=0)
    trg = test_data_all['status']
    feat = test_data_all.drop(columns=['status', 'code'], errors='ignore')

    N_1 = len([y for y in trg if y == 1])
    N_0 = len([y for y in trg if y == 0])

    def adjusted_precision(P, N, M, new_N, new_M):
        numerator = P * (new_N / N)
        denominator = numerator + (1 - P) * (new_M / M)
        return numerator / denominator

    def find_optimal_threshold_for_cluster(y_true, y_proba, metric='balanced_accuracy'):
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ."""
        from sklearn.metrics import balanced_accuracy_score, f1_score

        if len(np.unique(y_true)) < 2:
            return 0.5, 0.0

        # –î–∏–∞–ø–∞–∑–æ–Ω –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–µ–π
        proba_range = np.percentile(y_proba, [10, 90])
        thresholds = np.linspace(max(0.1, proba_range[0]),
                                 min(0.9, proba_range[1]), 50)

        best_threshold = 0.5
        best_score = 0.0

        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)

            if metric == 'balanced_accuracy':
                score = balanced_accuracy_score(y_true, y_pred)
            elif metric == 'f1':
                score = f1_score(y_true, y_pred, zero_division=0)
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞: {metric}")

            if score > best_score:
                best_score = score
                best_threshold = threshold

        return best_threshold, best_score

    def calculate_weighted_metrics(results_dict, metric_key):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É"""
        if not results_dict:
            return None, None, None

        total_weight = sum(r['weight'] for r in results_dict.values())
        weighted_sum = sum(r[metric_key] * r['weight'] for r in results_dict.values())
        weighted_avg = weighted_sum / total_weight if total_weight > 0 else 0

        # –¢–∞–∫–∂–µ –ø–æ—Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        simple_avg = np.mean([r[metric_key] for r in results_dict.values()])

        # –ú–µ–¥–∏–∞–Ω–∞ (–Ω–µ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è)
        median_val = np.median([r[metric_key] for r in results_dict.values()])

        return weighted_avg, simple_avg, median_val

    # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    category_results = {}
    cluster_results = {}

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú ====================
    cat_columns = [col for col in feat.columns if col.startswith('job_category_2_')]
    for cat_col in cat_columns:
        cat_name = cat_col.replace('job_category_2_', '')

        for seniority in [100]:
            mask = (feat[cat_col] == 1) & (feat['seniority'] < seniority)

            if mask.sum() > 100:
                predictions = _model.predict_proba(feat[mask])
                y_proba_united = predictions[:, 1]
                trg_filtered = trg[mask]

                if len(np.unique(trg_filtered)) > 1:
                    precision, recall, thresholds = precision_recall_curve(trg_filtered, y_proba_united)
                    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
                    optimal_idx = np.argmax(f1_scores)
                    optimal_threshold = thresholds[optimal_idx]

                    predictions_bin = (y_proba_united > optimal_threshold).astype(int)

                    f1_united = f1_score(trg_filtered, predictions_bin)
                    recall_united = recall_score(trg_filtered, predictions_bin)
                    precision_united = precision_score(trg_filtered, predictions_bin)
                    ba = balanced_accuracy_score(trg_filtered, predictions_bin)

                    category_results[cat_name] = {
                        'f1': f1_united,
                        'recall': recall_united,
                        'precision': precision_united,
                        'ba': ba,
                        'threshold': optimal_threshold,
                        'size': mask.sum(),
                        'weight': mask.sum() / len(feat),
                        'positive_rate': trg_filtered.mean(),
                        'pred_positive_rate': predictions_bin.mean()
                    }

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú ====================
    if feature_optimizer is not None:
        print(f'\n{"=" * 60}')
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú (–°–ú–ï–®–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï)")
        print(f'{"=" * 60}')

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê!
        if not hasattr(feature_optimizer, 'is_fitted'):
            print("‚ö†Ô∏è  –£ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ is_fitted!")
            # –ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—Ä—É–≥–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            if hasattr(feature_optimizer, 'cluster_info') and feature_optimizer.cluster_info:
                print("‚úÖ –ù–æ –µ—Å—Ç—å cluster_info, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
                feature_optimizer.is_fitted = True  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Ä—É—á–Ω—É—é
            else:
                print("‚ùå –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
                feature_optimizer = None

        elif not feature_optimizer.is_fitted:
            print("‚ö†Ô∏è  –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω (is_fitted=False)! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
            feature_optimizer = None

        if feature_optimizer is not None:
            predictions_all = _model.predict_proba(feat)
            y_proba_all = predictions_all[:, 1]

            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
                cluster_labels = feature_optimizer.predict_cluster(feat)
                unique_clusters = np.unique(cluster_labels)

                print(f"–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(unique_clusters)}")

                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
                cluster_stats = []
                for cluster_name in unique_clusters:
                    cluster_mask = (cluster_labels == cluster_name)
                    cluster_size = cluster_mask.sum()
                    cluster_stats.append((cluster_name, cluster_size))

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
                cluster_stats.sort(key=lambda x: x[1], reverse=True)

                print("\n–¢–æ–ø-10 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ —Ä–∞–∑–º–µ—Ä—É:")
                for i, (cluster_name, size) in enumerate(cluster_stats[:10]):
                    print(f"  {i + 1}. {cluster_name}: {size} –æ–±—ä–µ–∫—Ç–æ–≤ ({size / len(feat) * 100:.1f}%)")

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
                processed_clusters = 0
                for cluster_name in unique_clusters:
                    cluster_mask = (cluster_labels == cluster_name)

                    if cluster_mask.sum() > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä
                        y_proba_cluster = y_proba_all[cluster_mask]
                        trg_cluster = trg[cluster_mask]

                        if len(np.unique(trg_cluster)) > 1:
                            # 1. –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ—Ä–æ–≥ (–∏–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)
                            cluster_threshold = feature_optimizer.cluster_info.get(cluster_name, {}).get('threshold',
                                                                                                         0.5)
                            predictions_cluster = (y_proba_cluster > cluster_threshold).astype(int)

                            # –ú–µ—Ç—Ä–∏–∫–∏ —Å –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
                            if comparison_metric == 'balanced_accuracy':
                                score_cluster = balanced_accuracy_score(trg_cluster, predictions_cluster)
                            elif comparison_metric == 'f1':
                                score_cluster = f1_score(trg_cluster, predictions_cluster, zero_division=0)

                            # 2. –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ —Ç–æ–π –∂–µ –º–µ—Ç—Ä–∏–∫–µ
                            optimal_threshold, score_optimal = find_optimal_threshold_for_cluster(
                                trg_cluster, y_proba_cluster,
                                metric=comparison_metric
                            )
                            predictions_optimal = (y_proba_cluster > optimal_threshold).astype(int)

                            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                            f1_cluster = f1_score(trg_cluster, predictions_cluster, zero_division=0)
                            ba_cluster = balanced_accuracy_score(trg_cluster, predictions_cluster)
                            f1_optimal = f1_score(trg_cluster, predictions_optimal, zero_division=0)
                            ba_optimal = balanced_accuracy_score(trg_cluster, predictions_optimal)

                            cluster_results[cluster_name] = {
                                'score_cluster': score_cluster,
                                'score_optimal': score_optimal,
                                'f1_cluster': f1_cluster,
                                'f1_optimal': f1_optimal,
                                'ba_cluster': ba_cluster,
                                'ba_optimal': ba_optimal,
                                'cluster_threshold': cluster_threshold,
                                'optimal_threshold': optimal_threshold,
                                'size': cluster_mask.sum(),
                                'weight': cluster_mask.sum() / len(feat),
                                'positive_rate': trg_cluster.mean(),
                                'pred_positive_rate': predictions_cluster.mean(),
                                'improvement': score_optimal - score_cluster
                            }

                            processed_clusters += 1

                print(f"\n–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

                # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª—É—á—à–∏—Ö/—Ö—É–¥—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
                if cluster_results:
                    # –õ—É—á—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
                    sorted_by_improvement = sorted(
                        cluster_results.items(),
                        key=lambda x: x[1]['improvement'],
                        reverse=True
                    )[:5]

                    print(f"\n–¢–æ–ø-5 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é ({comparison_metric}):")
                    for cluster_name, metrics in sorted_by_improvement:
                        print(f"  {cluster_name}:")
                        print(f"    –†–∞–∑–º–µ—Ä: {metrics['size']} ({metrics['weight'] * 100:.1f}%)")
                        print(
                            f"    {comparison_metric}: {metrics['score_cluster']:.3f} -> {metrics['score_optimal']:.3f}")
                        print(f"    –£–ª—É—á—à–µ–Ω–∏–µ: {metrics['improvement']:+.3f}")
                        print(f"    –ü–æ—Ä–æ–≥: {metrics['cluster_threshold']:.3f} -> {metrics['optimal_threshold']:.3f}")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
                import traceback
                traceback.print_exc()
                cluster_results = {}

    # ==================== –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================
    print(f'\n{"=" * 60}')
    print(f"–°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í (–ú–µ—Ç—Ä–∏–∫–∞: {comparison_metric})")
    print(f'{"=" * 60}')

    results_summary = {}

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
    if category_results:
        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º BA (–∏–ª–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –ø–æ–¥ comparison_metric)
        ba_weighted, ba_simple, ba_median = calculate_weighted_metrics(category_results, 'ba')
        f1_weighted, f1_simple, f1_median = calculate_weighted_metrics(category_results, 'f1')

        results_summary['categories'] = {
            'ba_weighted': ba_weighted,
            'ba_simple': ba_simple,
            'f1_weighted': f1_weighted,
            'f1_simple': f1_simple,
            'n_groups': len(category_results),
            'coverage': sum(r['size'] for r in category_results.values()) / len(feat)
        }

        print(f"\nüìä –ö–ê–¢–ï–ì–û–†–ò–ò (job_category_2):")
        print(f"  –ì—Ä—É–ø–ø: {len(category_results)}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö: {results_summary['categories']['coverage'] * 100:.1f}%")
        print(f"  Balanced Accuracy:")
        print(f"    –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_weighted:.4f}")
        print(f"    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_simple:.4f}")
        print(f"  F1-score:")
        print(f"    –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π: {f1_weighted:.4f}")
        print(f"    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {f1_simple:.4f}")

    # –ö–ª–∞—Å—Ç–µ—Ä—ã
    if cluster_results:
        if comparison_metric == 'balanced_accuracy':
            metric_key = 'ba_cluster'
            metric_key_optimal = 'ba_optimal'
        else:  # 'f1'
            metric_key = 'f1_cluster'
            metric_key_optimal = 'f1_optimal'

        # –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        score_cluster_w, score_cluster_s, _ = calculate_weighted_metrics(
            cluster_results, 'score_cluster'
        )
        # –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        score_optimal_w, score_optimal_s, _ = calculate_weighted_metrics(
            cluster_results, 'score_optimal'
        )

        # –¢–∞–∫–∂–µ BA –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        ba_cluster_w, ba_cluster_s, _ = calculate_weighted_metrics(cluster_results, 'ba_cluster')
        ba_optimal_w, ba_optimal_s, _ = calculate_weighted_metrics(cluster_results, 'ba_optimal')

        results_summary['clusters'] = {
            'score_cluster_weighted': score_cluster_w,
            'score_cluster_simple': score_cluster_s,
            'score_optimal_weighted': score_optimal_w,
            'score_optimal_simple': score_optimal_s,
            'ba_cluster_weighted': ba_cluster_w,
            'ba_cluster_simple': ba_cluster_s,
            'ba_optimal_weighted': ba_optimal_w,
            'ba_optimal_simple': ba_optimal_s,
            'n_groups': len(cluster_results),
            'coverage': sum(r['size'] for r in cluster_results.values()) / len(feat),
            'avg_improvement': np.mean([r['improvement'] for r in cluster_results.values()])
        }

        print(f"\nüìä –ö–õ–ê–°–¢–ï–†–´ (MixedDataClusterOptimizer):")
        print(f"  –ì—Ä—É–ø–ø: {len(cluster_results)}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö: {results_summary['clusters']['coverage'] * 100:.1f}%")
        print(f"  –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ ({comparison_metric}):")
        print(f"    –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {score_cluster_w:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {score_cluster_s:.4f}")
        print(f"    –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {score_optimal_w:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {score_optimal_s:.4f}")
        print(f"    –£–ª—É—á—à–µ–Ω–∏–µ: {score_optimal_w - score_cluster_w:+.4f}")
        print(f"    –°—Ä–µ–¥–Ω–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {results_summary['clusters']['avg_improvement']:+.4f}")

        # Balanced Accuracy –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã –∫–∞—Ä—Ç–∏–Ω—ã
        print(f"\n  Balanced Accuracy (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è):")
        print(f"    –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏: {ba_cluster_w:.4f}")
        print(f"    –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏: {ba_optimal_w:.4f}")

    # ==================== –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í ====================
    if category_results and cluster_results:
        print(f'\n{"=" * 60}')
        print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô –ò –ö–õ–ê–°–¢–ï–†–û–í")
        print(f'{"=" * 60}')

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ
        if comparison_metric == 'balanced_accuracy':
            categories_score = results_summary['categories']['ba_weighted']
            clusters_score = results_summary['clusters']['score_optimal_weighted']
        else:  # 'f1'
            categories_score = results_summary['categories']['f1_weighted']
            clusters_score = results_summary['clusters']['score_optimal_weighted']

        improvement = clusters_score - categories_score

        print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ {comparison_metric}:")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:    {categories_score:.4f}")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ã:     {clusters_score:.4f}")
        print(f"  –†–∞–∑–Ω–∏—Ü–∞:      {improvement:+.4f}")

        # –¢–∞–∫–∂–µ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ BA
        categories_ba = results_summary['categories']['ba_weighted']
        clusters_ba = results_summary['clusters']['ba_optimal_weighted']
        improvement_ba = clusters_ba - categories_ba

        print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ Balanced Accuracy:")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:    {categories_ba:.4f}")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ã:     {clusters_ba:.4f}")
        print(f"  –†–∞–∑–Ω–∏—Ü–∞:      {improvement_ba:+.4f}")

        # –§–æ—Ä–º—É–ª–∏—Ä—É–µ–º –≤—ã–≤–æ–¥
        if improvement > 0.01:
            print(f"\n‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –°–£–©–ï–°–¢–í–ï–ù–ù–û —É–ª—É—á—à–∞–µ—Ç {comparison_metric}!")
        elif improvement > 0:
            print(f"\n‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ù–ï–ú–ù–û–ì–û —É–ª—É—á—à–∞–µ—Ç {comparison_metric}")
        elif improvement > -0.01:
            print(f"\n‚ö†Ô∏è  –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ù–ï –£–•–£–î–®–ê–ï–¢ {comparison_metric}")
        else:
            print(f"\n‚ùå –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –£–•–£–î–®–ê–ï–¢ {comparison_metric}")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
        if improvement > 0:
            print(f"\nüèÜ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –ø–æ—Ä–æ–≥–∏")
            print(f"   –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.4f} –ø–æ {comparison_metric}")
        else:
            print(f"\nüèÜ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏")
            print(f"   –ö–ª–∞—Å—Ç–µ—Ä—ã –Ω–µ –¥–∞—é—Ç —É–ª—É—á—à–µ–Ω–∏—è")

    # ==================== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ====================
    if cluster_results:
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            print(f'\n{"=" * 60}')
            print("–í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
            print(f'{"=" * 60}')

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
            clusters_df = pd.DataFrame.from_dict(cluster_results, orient='index')
            clusters_df = clusters_df.sort_values('size', ascending=False)

            fig, axes = plt.subplots(2, 3, figsize=(18, 12))

            # 1. –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            axes[0, 0].bar(range(len(clusters_df)), clusters_df['size'].values)
            axes[0, 0].set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä—ã (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É)')
            axes[0, 0].set_ylabel('–†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞')
            axes[0, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
            axes[0, 0].tick_params(axis='x', rotation=45)

            # 2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            axes[0, 1].scatter(range(len(clusters_df)),
                               clusters_df['score_cluster'].values,
                               alpha=0.6, label='–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –ø–æ—Ä–æ–≥–∏', s=50)
            axes[0, 1].scatter(range(len(clusters_df)),
                               clusters_df['score_optimal'].values,
                               alpha=0.6, label='–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏', s=50)
            axes[0, 1].set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä—ã')
            axes[0, 1].set_ylabel(f'{comparison_metric.upper()}')
            axes[0, 1].set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

            # 3. –£–ª—É—á—à–µ–Ω–∏–µ vs —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
            axes[0, 2].scatter(clusters_df['size'].values,
                               clusters_df['improvement'].values,
                               alpha=0.6, s=50)
            axes[0, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
            axes[0, 2].set_xlabel('–†–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞')
            axes[0, 2].set_ylabel('–£–ª—É—á—à–µ–Ω–∏–µ')
            axes[0, 2].set_title('–£–ª—É—á—à–µ–Ω–∏–µ vs —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞')
            axes[0, 2].set_xscale('log')
            axes[0, 2].grid(True, alpha=0.3)

            # 4. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤
            axes[1, 0].hist(clusters_df['cluster_threshold'].values,
                            alpha=0.5, label='–ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ', bins=15)
            axes[1, 0].hist(clusters_df['optimal_threshold'].values,
                            alpha=0.5, label='–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ', bins=15)
            axes[1, 0].set_xlabel('–ü–æ—Ä–æ–≥')
            axes[1, 0].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
            axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–æ–≤')
            axes[1, 0].legend()
            axes[1, 0].axvline(x=0.5, color='red', linestyle='--', alpha=0.5)

            # 5. –ö–∞—á–µ—Å—Ç–≤–æ vs –¥–æ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞
            axes[1, 1].scatter(clusters_df['positive_rate'].values,
                               clusters_df['score_optimal'].values,
                               alpha=0.6, s=50)
            axes[1, 1].set_xlabel('–î–æ–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞')
            axes[1, 1].set_ylabel(f'{comparison_metric.upper()}')
            axes[1, 1].set_title('–ö–∞—á–µ—Å—Ç–≤–æ vs –¥–∏—Å–±–∞–ª–∞–Ω—Å')
            axes[1, 1].grid(True, alpha=0.3)

            # 6. –†–∞–∑–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–≥–æ–≤ vs —É–ª—É—á—à–µ–Ω–∏–µ
            threshold_diff = clusters_df['optimal_threshold'] - clusters_df['cluster_threshold']
            axes[1, 2].scatter(threshold_diff.values,
                               clusters_df['improvement'].values,
                               alpha=0.6, s=50)
            axes[1, 2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
            axes[1, 2].axvline(x=0, color='red', linestyle='--', alpha=0.5)
            axes[1, 2].set_xlabel('–†–∞–∑–Ω–æ—Å—Ç—å –ø–æ—Ä–æ–≥–æ–≤ (–æ–ø—Ç–∏–º - –∫–ª–∞—Å—Ç–µ—Ä)')
            axes[1, 2].set_ylabel('–£–ª—É—á—à–µ–Ω–∏–µ')
            axes[1, 2].set_title('–í–ª–∏—è–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ—Ä–æ–≥–∞ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ')
            axes[1, 2].grid(True, alpha=0.3)

            plt.tight_layout()
            plt.show()

        except Exception as e:
            print(f"\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")

    return {
        'category_results': category_results,
        'cluster_results': cluster_results,
        'results_summary': results_summary,
        'comparison_metric': comparison_metric
    }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
def test_with_mixed_clusters(model, train_data, test_data):
    """
    –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å MixedDataClusterOptimizer
    """

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train = train_data.drop(columns=['status', 'code'], errors='ignore')
    y_train = train_data['status']

    X_test = [test_data] if isinstance(test_data, pd.DataFrame) else test_data

    # 2. –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º MixedDataClusterOptimizer
    print("–°–æ–∑–¥–∞–Ω–∏–µ MixedDataClusterOptimizer...")
    mixed_optimizer = MixedDataClusterOptimizer(
        n_clusters=25,
        min_cluster_size=100,
        categorical_distance='jaccard',
        metric='balanced_accuracy',
        random_state=42
    )

    # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç—Ä–µ–π–Ω–µ
    y_pred_proba_train = model.predict_proba(X_train)[:, 1]

    # –û–±—É—á–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä
    print("–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ç–æ—Ä–∞...")
    mixed_optimizer.fit(
        X=X_train,
        y=y_train,
        y_pred_proba=y_pred_proba_train
    )

    # 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    results = test_with_clusters_weighted_advanced(
        _model=model,
        _test_data=X_test,
        _cat_features=[],  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ —É—á—Ç–µ–Ω—ã –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–µ
        feature_optimizer=mixed_optimizer,
        comparison_metric='balanced_accuracy',
        _inference=False
    )

    return results, mixed_optimizer



def test_with_clusters_weighted(_model, _test_data, _cat_features,
                                         cluster_optimizer=None,
                                         feature_optimizer=None,
                                         _inference=False):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ (—Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏).

    :param _model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å CatBoost
    :param _test_data: —Å–ø–∏—Å–æ–∫ DataFrame —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    :param _cat_features: —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :param cluster_optimizer: –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π LeafClusterThresholdOptimizer (–ø–æ –ª–∏—Å—Ç—å—è–º)
    :param feature_optimizer: –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π FeatureClusterThresholdOptimizer (–ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º)
    :param _inference: —Ñ–ª–∞–≥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    """

    pos_class = 700
    neg_class = 5500

    test_data_all = pd.concat(_test_data, axis=0)
    trg = test_data_all['status']
    feat = test_data_all.drop(columns=['status', 'code'], errors='ignore')

    N_1 = len([y for y in trg if y == 1])
    N_0 = len([y for y in trg if y == 0])

    def adjusted_precision(P, N, M, new_N, new_M):
        numerator = P * (new_N / N)
        denominator = numerator + (1 - P) * (new_M / M)
        return numerator / denominator

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    def calculate_weighted_metrics(results_dict, metric_key):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É"""
        if not results_dict:
            return None, None, None

        total_weight = sum(r['weight'] for r in results_dict.values())
        weighted_sum = sum(r[metric_key] * r['weight'] for r in results_dict.values())
        weighted_avg = weighted_sum / total_weight if total_weight > 0 else 0

        # –¢–∞–∫–∂–µ –ø–æ—Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        simple_avg = np.mean([r[metric_key] for r in results_dict.values()])

        # –ú–µ–¥–∏–∞–Ω–∞ (–Ω–µ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è)
        median_val = np.median([r[metric_key] for r in results_dict.values()])

        return weighted_avg, simple_avg, median_val

    # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    category_results = {}
    leaf_cluster_results = {}
    feature_cluster_results = {}

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú ====================
    cat_columns = [col for col in feat.columns if col.startswith('job_category_2_')]
    for cat_col in cat_columns:
        cat_name = cat_col.replace('job_category_2_', '')

        for seniority in [100]:
            mask = (feat[cat_col] == 1) & (feat['seniority'] < seniority)

            if mask.sum() > 100:
                predictions = _model.predict_proba(feat[mask])
                y_proba_united = predictions[:, 1]
                trg_filtered = trg[mask]

                if len(np.unique(trg_filtered)) > 1:
                    precision, recall, thresholds = precision_recall_curve(trg_filtered, y_proba_united)
                    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
                    optimal_idx = np.argmax(f1_scores)
                    optimal_threshold = thresholds[optimal_idx]

                    predictions_bin = (y_proba_united > optimal_threshold).astype(int)

                    f1_united = f1_score(trg_filtered, predictions_bin)
                    recall_united = recall_score(trg_filtered, predictions_bin)
                    precision_united = precision_score(trg_filtered, predictions_bin)
                    ba = balanced_accuracy_score(trg_filtered, predictions_bin)

                    category_results[cat_name] = {
                        'f1': f1_united,
                        'recall': recall_united,
                        'precision': precision_united,
                        'ba': ba,
                        'threshold': optimal_threshold,
                        'size': mask.sum(),
                        'weight': mask.sum() / len(feat),
                        'positive_rate': trg_filtered.mean(),
                        'pred_positive_rate': predictions_bin.mean()
                    }

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú (–õ–ò–°–¢–¨–Ø) ====================
    if cluster_optimizer is not None:
        print(f'\n{"=" * 60}')
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú (–õ–ò–°–¢–¨–Ø –î–ï–†–ï–í–¨–ï–í)")
        print(f'{"=" * 60}')

        predictions_all = _model.predict_proba(feat)
        y_proba_all = predictions_all[:, 1]
        cluster_labels = cluster_optimizer.predict_cluster(feat)
        unique_clusters = np.unique(cluster_labels)

        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(unique_clusters)}")

        for cluster_name in unique_clusters:
            cluster_mask = (cluster_labels == cluster_name)

            if cluster_mask.sum() > 10:
                y_proba_cluster = y_proba_all[cluster_mask]
                trg_cluster = trg[cluster_mask]

                if len(np.unique(trg_cluster)) > 1:
                    # –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ—Ä–æ–≥
                    cluster_threshold = cluster_optimizer.cluster_info.get(cluster_name, {}).get('threshold', 0.5)
                    predictions_cluster = (y_proba_cluster > cluster_threshold).astype(int)

                    f1_cluster = f1_score(trg_cluster, predictions_cluster)
                    ba_cluster = balanced_accuracy_score(trg_cluster, predictions_cluster)

                    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    precision_curve, recall_curve, thresholds_curve = precision_recall_curve(
                        trg_cluster, y_proba_cluster
                    )
                    f1_scores_curve = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)

                    if len(f1_scores_curve) > 0:
                        optimal_idx = np.argmax(f1_scores_curve)
                        optimal_threshold = thresholds_curve[optimal_idx]
                        predictions_optimal = (y_proba_cluster > optimal_threshold).astype(int)
                        f1_optimal = f1_score(trg_cluster, predictions_optimal)
                        ba_optimal = balanced_accuracy_score(trg_cluster, predictions_optimal)
                    else:
                        optimal_threshold = cluster_threshold
                        f1_optimal = f1_cluster
                        ba_optimal = ba_cluster

                    leaf_cluster_results[cluster_name] = {
                        'f1_cluster_thresh': f1_cluster,
                        'f1_optimal_thresh': f1_optimal,
                        'ba_cluster_thresh': ba_cluster,
                        'ba_optimal_thresh': ba_optimal,
                        'cluster_threshold': cluster_threshold,
                        'optimal_threshold': optimal_threshold,
                        'size': cluster_mask.sum(),
                        'weight': cluster_mask.sum() / len(feat),
                        'positive_rate': trg_cluster.mean(),
                        'pred_positive_rate': predictions_cluster.mean()
                    }

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú (–ü–†–ò–ó–ù–ê–ö–ò) ====================
    if feature_optimizer is not None:
        print(f'\n{"=" * 60}')
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú (–ü–†–ò–ó–ù–ê–ö–ò K-MEANS)")
        print(f'{"=" * 60}')

        predictions_all = _model.predict_proba(feat)
        y_proba_all = predictions_all[:, 1]
        cluster_labels = feature_optimizer.predict_cluster(feat)
        unique_clusters = np.unique(cluster_labels)

        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(unique_clusters)}")

        for cluster_name in unique_clusters:
            cluster_mask = (cluster_labels == cluster_name)

            if cluster_mask.sum() > 10:
                y_proba_cluster = y_proba_all[cluster_mask]
                trg_cluster = trg[cluster_mask]

                if len(np.unique(trg_cluster)) > 1:
                    # –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ—Ä–æ–≥
                    cluster_threshold = feature_optimizer.cluster_info.get(cluster_name, {}).get('threshold', 0.5)
                    predictions_cluster = (y_proba_cluster > cluster_threshold).astype(int)

                    f1_cluster = f1_score(trg_cluster, predictions_cluster)
                    ba_cluster = balanced_accuracy_score(trg_cluster, predictions_cluster)

                    # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                    precision_curve, recall_curve, thresholds_curve = precision_recall_curve(
                        trg_cluster, y_proba_cluster
                    )
                    f1_scores_curve = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)

                    if len(f1_scores_curve) > 0:
                        optimal_idx = np.argmax(f1_scores_curve)
                        optimal_threshold = thresholds_curve[optimal_idx]
                        predictions_optimal = (y_proba_cluster > optimal_threshold).astype(int)
                        f1_optimal = f1_score(trg_cluster, predictions_optimal)
                        ba_optimal = balanced_accuracy_score(trg_cluster, predictions_optimal)
                    else:
                        optimal_threshold = cluster_threshold
                        f1_optimal = f1_cluster
                        ba_optimal = ba_cluster

                    feature_cluster_results[cluster_name] = {
                        'f1_cluster_thresh': f1_cluster,
                        'f1_optimal_thresh': f1_optimal,
                        'ba_cluster_thresh': ba_cluster,
                        'ba_optimal_thresh': ba_optimal,
                        'cluster_threshold': cluster_threshold,
                        'optimal_threshold': optimal_threshold,
                        'size': cluster_mask.sum(),
                        'weight': cluster_mask.sum() / len(feat),
                        'positive_rate': trg_cluster.mean(),
                        'pred_positive_rate': predictions_cluster.mean()
                    }

    # ==================== –í–´–í–û–î –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ====================
    print(f'\n{"=" * 60}')
    print("–°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ü–û–î–•–û–î–û–í (–°–†–ï–î–ù–ï–í–ó–í–ï–®–ï–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò)")
    print(f'{"=" * 60}')

    results_summary = {}

    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
    if category_results:
        ba_weighted, ba_simple, ba_median = calculate_weighted_metrics(category_results, 'ba')
        f1_weighted, f1_simple, f1_median = calculate_weighted_metrics(category_results, 'f1')

        results_summary['categories'] = {
            'ba_weighted': ba_weighted,
            'ba_simple': ba_simple,
            'f1_weighted': f1_weighted,
            'f1_simple': f1_simple,
            'n_groups': len(category_results),
            'coverage': sum(r['size'] for r in category_results.values()) / len(feat)
        }

        print(f"\nüìä –ö–ê–¢–ï–ì–û–†–ò–ò (job_category_2):")
        print(f"  –ì—Ä—É–ø–ø: {len(category_results)}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö: {results_summary['categories']['coverage'] * 100:.1f}%")
        print(f"  Balanced Accuracy:")
        print(f"    –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_weighted:.4f}")
        print(f"    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_simple:.4f}")
        print(f"  F1-score:")
        print(f"    –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π: {f1_weighted:.4f}")
        print(f"    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {f1_simple:.4f}")

    # –ö–ª–∞—Å—Ç–µ—Ä—ã –ø–æ –ª–∏—Å—Ç—å—è–º
    if leaf_cluster_results:
        ba_cluster_w, ba_cluster_s, _ = calculate_weighted_metrics(leaf_cluster_results, 'ba_cluster_thresh')
        ba_optimal_w, ba_optimal_s, _ = calculate_weighted_metrics(leaf_cluster_results, 'ba_optimal_thresh')

        results_summary['leaf_clusters'] = {
            'ba_cluster_weighted': ba_cluster_w,
            'ba_cluster_simple': ba_cluster_s,
            'ba_optimal_weighted': ba_optimal_w,
            'ba_optimal_simple': ba_optimal_s,
            'n_groups': len(leaf_cluster_results),
            'coverage': sum(r['size'] for r in leaf_cluster_results.values()) / len(feat)
        }

        print(f"\nüìä –ö–õ–ê–°–¢–ï–†–´ –ü–û –õ–ò–°–¢–¨–Ø–ú:")
        print(f"  –ì—Ä—É–ø–ø: {len(leaf_cluster_results)}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö: {results_summary['leaf_clusters']['coverage'] * 100:.1f}%")
        print(f"  Balanced Accuracy:")
        print(f"    –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_cluster_w:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_cluster_s:.4f}")
        print(f"    –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_optimal_w:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_optimal_s:.4f}")
        print(f"    –£–ª—É—á—à–µ–Ω–∏–µ (–æ–ø—Ç–∏–º vs –∫–ª–∞—Å—Ç–µ—Ä): {ba_optimal_w - ba_cluster_w:+.4f}")

    # –ö–ª–∞—Å—Ç–µ—Ä—ã –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    if feature_cluster_results:
        ba_cluster_w, ba_cluster_s, _ = calculate_weighted_metrics(feature_cluster_results, 'ba_cluster_thresh')
        ba_optimal_w, ba_optimal_s, _ = calculate_weighted_metrics(feature_cluster_results, 'ba_optimal_thresh')

        results_summary['feature_clusters'] = {
            'ba_cluster_weighted': ba_cluster_w,
            'ba_cluster_simple': ba_cluster_s,
            'ba_optimal_weighted': ba_optimal_w,
            'ba_optimal_simple': ba_optimal_s,
            'n_groups': len(feature_cluster_results),
            'coverage': sum(r['size'] for r in feature_cluster_results.values()) / len(feat)
        }

        print(f"\nüìä –ö–õ–ê–°–¢–ï–†–´ –ü–û –ü–†–ò–ó–ù–ê–ö–ê–ú (K-means):")
        print(f"  –ì—Ä—É–ø–ø: {len(feature_cluster_results)}")
        print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ –¥–∞–Ω–Ω—ã—Ö: {results_summary['feature_clusters']['coverage'] * 100:.1f}%")
        print(f"  Balanced Accuracy:")
        print(f"    –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_cluster_w:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_cluster_s:.4f}")
        print(f"    –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_optimal_w:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_optimal_s:.4f}")
        print(f"    –£–ª—É—á—à–µ–Ω–∏–µ (–æ–ø—Ç–∏–º vs –∫–ª–∞—Å—Ç–µ—Ä): {ba_optimal_w - ba_cluster_w:+.4f}")

    # ==================== –°–†–ê–í–ù–ï–ù–ò–ï –ü–û–î–•–û–î–û–í ====================
    print(f'\n{"=" * 60}')
    print("–ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï")
    print(f'{"=" * 60}')

    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    comparison_data = []

    if 'categories' in results_summary:
        comparison_data.append({
            'Method': 'Categories',
            'BA_weighted': results_summary['categories']['ba_weighted'],
            'F1_weighted': results_summary['categories']['f1_weighted'],
            'N_groups': results_summary['categories']['n_groups'],
            'Coverage': results_summary['categories']['coverage']
        })

    if 'leaf_clusters' in results_summary:
        comparison_data.append({
            'Method': 'Leaf Clusters',
            'BA_weighted': results_summary['leaf_clusters']['ba_optimal_weighted'],
            'F1_weighted': None,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            'N_groups': results_summary['leaf_clusters']['n_groups'],
            'Coverage': results_summary['leaf_clusters']['coverage']
        })

    if 'feature_clusters' in results_summary:
        comparison_data.append({
            'Method': 'Feature Clusters',
            'BA_weighted': results_summary['feature_clusters']['ba_optimal_weighted'],
            'F1_weighted': None,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            'N_groups': results_summary['feature_clusters']['n_groups'],
            'Coverage': results_summary['feature_clusters']['coverage']
        })

    if comparison_data:
        comparison_df = pd.DataFrame(comparison_data)
        print(f"\n{comparison_df.to_string(index=False)}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –º–µ—Ç–æ–¥
        if len(comparison_df) > 1:
            best_method = comparison_df.loc[comparison_df['BA_weighted'].idxmax(), 'Method']
            best_ba = comparison_df['BA_weighted'].max()

            print(f"\nüèÜ –õ–£–ß–®–ò–ô –ú–ï–¢–û–î: {best_method}")
            print(f"   Balanced Accuracy: {best_ba:.4f}")

            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
            if 'categories' in results_summary:
                categories_ba = results_summary['categories']['ba_weighted']
                improvement = best_ba - categories_ba

                if improvement > 0.01:
                    print(f"   ‚úÖ –£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {improvement:+.4f}")
                elif improvement > 0:
                    print(f"   ‚ö†Ô∏è  –ù–µ–±–æ–ª—å—à–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.4f}")
                else:
                    print(f"   ‚ùå –£—Ö—É–¥—à–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {improvement:+.4f}")

    return {
        'category_results': category_results,
        'leaf_cluster_results': leaf_cluster_results,
        'feature_cluster_results': feature_cluster_results,
        'results_summary': results_summary,
        'comparison_df': comparison_df if 'comparison_df' in locals() else None
    }


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
# if __name__ == "__main__":
#     # –í–∞—à –∫–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
#
#     # 1. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)
#     results = test_with_clusters_weighted_advanced(
#         _model=model,
#         _test_data=test_data_list,
#         _cat_features=cat_features,
#         cluster_optimizer=None,
#         feature_optimizer=None
#     )
#
#     # 2. –° –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –ø–æ –ª–∏—Å—Ç—å—è–º
#     if False:  # –ú–æ–∂–µ—Ç–µ –≤–∫–ª—é—á–∏—Ç—å –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
#         leaf_optimizer = LeafClusterThresholdOptimizer(
#             model=model,
#             n_trees=2,
#             min_cluster_size=50
#         )
#
#         y_pred_proba_train = model.predict_proba(X_train)[:, 1]
#         leaf_optimizer.fit(X_train, y_train, y_pred_proba_train)
#
#         results = test_with_clusters_weighted_advanced(
#             _model=model,
#             _test_data=test_data_list,
#             _cat_features=cat_features,
#             cluster_optimizer=leaf_optimizer,
#             feature_optimizer=None
#         )
#
#     # 3. –° –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô –í–ê–†–ò–ê–ù–¢)
#     feature_optimizer = FeatureClusterThresholdOptimizer(
#         n_clusters=30,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
#         min_cluster_size=50,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
#         use_pca=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PCA
#         pca_components=20,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç PCA
#         random_state=42
#     )
#
#     y_pred_proba_train = model.predict_proba(X_train)[:, 1]
#     feature_optimizer.fit(X_train, y_train, y_pred_proba_train)
#
#     results = test_with_clusters_weighted_advanced(
#         _model=model,
#         _test_data=test_data_list,
#         _cat_features=cat_features,
#         cluster_optimizer=None,
#         feature_optimizer=feature_optimizer
#     )


def test_with_leaf_clusters_weighted(_model, _test_data, _cat_features, optimizer=None, _inference=False):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ –∏ —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏

    :param _model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å CatBoost
    :param _test_data: —Å–ø–∏—Å–æ–∫ DataFrame —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    :param _cat_features: —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    :param optimizer: –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π LeafClusterThresholdOptimizer
    :param _inference: —Ñ–ª–∞–≥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    """

    pos_class = 700
    neg_class = 5500

    test_data_all = pd.concat(_test_data, axis=0)
    trg = test_data_all['status']
    feat = test_data_all.drop(columns=['code'], errors='ignore')

    N_1 = len([y for y in trg if y == 1])
    N_0 = len([y for y in trg if y == 0])

    def adjusted_precision(P, N, M, new_N, new_M):
        numerator = P * (new_N / N)
        denominator = numerator + (1 - P) * (new_M / M)
        return numerator / denominator

    # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤–µ—Å–∞–º–∏
    category_results = {}
    cluster_results = {}

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú ====================
    cat_columns = [col for col in feat.columns if col.startswith('job_category_2_')]
    for cat_col in cat_columns:
        cat_name = cat_col.replace('job_category_2_', '')

        for seniority in [100]:
            # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ seniority
            mask = (feat[cat_col] == 1) & (feat['seniority'] < seniority)

            if mask.sum() > 100:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                predictions = _model.predict_proba(feat[mask].drop(columns=['status']))
                y_proba_united = predictions[:, 1]
                trg_filtered = trg[mask]

                if len(np.unique(trg_filtered)) > 1:
                    # –ü–æ–¥–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ F1
                    precision, recall, thresholds = precision_recall_curve(trg_filtered, y_proba_united)
                    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)
                    optimal_idx = np.argmax(f1_scores)
                    optimal_threshold = thresholds[optimal_idx]

                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
                    predictions_bin = (y_proba_united > optimal_threshold).astype(int)

                    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    f1_united = f1_score(trg_filtered, predictions_bin)
                    recall_united = recall_score(trg_filtered, predictions_bin)
                    precision_united = precision_score(trg_filtered, predictions_bin)
                    ba = balanced_accuracy_score(trg_filtered, predictions_bin)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–æ–º (—Ä–∞–∑–º–µ—Ä–æ–º –≤—ã–±–æ—Ä–∫–∏)
                    category_results[cat_name] = {
                        'f1': f1_united,
                        'recall': recall_united,
                        'precision': precision_united,
                        'ba': ba,
                        'threshold': optimal_threshold,
                        'size': mask.sum(),
                        'weight': mask.sum() / len(feat),  # –î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                        'positive_rate': trg_filtered.mean(),
                        'pred_positive_rate': predictions_bin.mean()
                    }

    # ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú ====================
    if optimizer is not None:
        print(f'\n{"=" * 60}')
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú")
        print(f'{"=" * 60}')

        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        predictions_all = _model.predict_proba(feat.drop(columns=['status']))
        y_proba_all = predictions_all[:, 1]

        # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        cluster_labels = optimizer.predict_cluster(feat.drop(columns=['status']))

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
        unique_clusters = np.unique(cluster_labels)
        print(f"–ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(unique_clusters)}")

        for cluster_name in unique_clusters:
            # –ú–∞—Å–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            cluster_mask = (cluster_labels == cluster_name)

            if cluster_mask.sum() > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
                # –î–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
                y_proba_cluster = y_proba_all[cluster_mask]
                trg_cluster = trg[cluster_mask]

                if len(np.unique(trg_cluster)) > 1:
                    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Ä–æ–≥ –∏–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
                    cluster_threshold = optimizer.cluster_info.get(cluster_name, {}).get('threshold', 0.5)
                    predictions_cluster = (y_proba_cluster > cluster_threshold).astype(int)

                    # –ú–µ—Ç—Ä–∏–∫–∏ —Å –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
                    f1_cluster = f1_score(trg_cluster, predictions_cluster)
                    ba_cluster = balanced_accuracy_score(trg_cluster, predictions_cluster)

                    # –í–∞—Ä–∏–∞–Ω—Ç 2: –ü–æ–¥–æ–±—Ä–∞—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –ø–æ F1 –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
                    precision_curve, recall_curve, thresholds_curve = precision_recall_curve(
                        trg_cluster, y_proba_cluster
                    )
                    f1_scores_curve = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-9)
                    if len(f1_scores_curve) > 0:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏
                        optimal_idx = np.argmax(f1_scores_curve)
                        optimal_threshold = thresholds_curve[optimal_idx]

                        predictions_optimal = (y_proba_cluster > optimal_threshold).astype(int)
                        f1_optimal = f1_score(trg_cluster, predictions_optimal)
                        ba_optimal = balanced_accuracy_score(trg_cluster, predictions_optimal)
                    else:
                        optimal_threshold = cluster_threshold
                        f1_optimal = f1_cluster
                        ba_optimal = ba_cluster

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–æ–º
                    cluster_results[cluster_name] = {
                        'f1_cluster_thresh': f1_cluster,
                        'f1_optimal_thresh': f1_optimal,
                        'ba_cluster_thresh': ba_cluster,
                        'ba_optimal_thresh': ba_optimal,
                        'cluster_threshold': cluster_threshold,
                        'optimal_threshold': optimal_threshold,
                        'size': cluster_mask.sum(),
                        'weight': cluster_mask.sum() / len(feat),  # –î–æ–ª—è –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                        'positive_rate': trg_cluster.mean(),
                        'pred_positive_rate': predictions_cluster.mean()
                    }

    # ==================== –†–ê–°–ß–ï–¢ –°–†–ï–î–ù–ï–í–ó–í–ï–®–ï–ù–ù–´–• –ú–ï–¢–†–ò–ö ====================
    print(f'\n{"=" * 60}')
    print("–°–†–ï–î–ù–ï–í–ó–í–ï–®–ï–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò")
    print(f'{"=" * 60}')

    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
    def calculate_weighted_metrics(results_dict, metric_key):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—É—é –º–µ—Ç—Ä–∏–∫—É"""
        if not results_dict:
            return None, None, None

        total_weight = sum(r['weight'] for r in results_dict.values())
        weighted_sum = sum(r[metric_key] * r['weight'] for r in results_dict.values())
        weighted_avg = weighted_sum / total_weight if total_weight > 0 else 0

        # –¢–∞–∫–∂–µ –ø–æ—Å—á–∏—Ç–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        simple_avg = np.mean([r[metric_key] for r in results_dict.values()])

        # –ú–µ–¥–∏–∞–Ω–∞ (–Ω–µ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è)
        median_val = np.median([r[metric_key] for r in results_dict.values()])

        return weighted_avg, simple_avg, median_val

    # –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    if category_results:
        ba_weighted_cat, ba_simple_cat, ba_median_cat = calculate_weighted_metrics(
            category_results, 'ba'
        )
        f1_weighted_cat, f1_simple_cat, f1_median_cat = calculate_weighted_metrics(
            category_results, 'f1'
        )

        print(f"\nüìä –ö–ê–¢–ï–ì–û–†–ò–ò (–≤—Å–µ–≥–æ {len(category_results)}):")
        print(f"  Balanced Accuracy:")
        print(f"    –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_weighted_cat:.4f}")
        print(f"    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_simple_cat:.4f}")
        print(f"    –ú–µ–¥–∏–∞–Ω–∞:          {ba_median_cat:.4f}")
        print(f"  F1-score:")
        print(f"    –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π: {f1_weighted_cat:.4f}")
        print(f"    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {f1_simple_cat:.4f}")
        print(f"    –ú–µ–¥–∏–∞–Ω–∞:          {f1_median_cat:.4f}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        cat_sizes = [r['size'] for r in category_results.values()]
        print(f"\n  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π:")
        print(f"    –ú–∏–Ω–∏–º—É–º:    {min(cat_sizes)}")
        print(f"    –ú–∞–∫—Å–∏–º—É–º:   {max(cat_sizes)}")
        print(f"    –ú–µ–¥–∏–∞–Ω–∞:    {np.median(cat_sizes):.0f}")
        print(f"    75-–π –ø–µ—Ä—Ü.: {np.percentile(cat_sizes, 75):.0f}")

    # –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    if cluster_results:
        # –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        ba_weighted_cluster, ba_simple_cluster, ba_median_cluster = calculate_weighted_metrics(
            cluster_results, 'ba_cluster_thresh'
        )
        f1_weighted_cluster, f1_simple_cluster, f1_median_cluster = calculate_weighted_metrics(
            cluster_results, 'f1_cluster_thresh'
        )

        # –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
        ba_weighted_optimal, ba_simple_optimal, ba_median_optimal = calculate_weighted_metrics(
            cluster_results, 'ba_optimal_thresh'
        )
        f1_weighted_optimal, f1_simple_optimal, f1_median_optimal = calculate_weighted_metrics(
            cluster_results, 'f1_optimal_thresh'
        )

        print(f"\nüìä –ö–õ–ê–°–¢–ï–†–´ (–≤—Å–µ–≥–æ {len(cluster_results)}):")
        print(f"\n  –° –ö–õ–ê–°–¢–ï–†–ù–´–ú–ò –ü–û–†–û–ì–ê–ú–ò:")
        print(f"    Balanced Accuracy:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_weighted_cluster:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_simple_cluster:.4f}")
        print(f"      –ú–µ–¥–∏–∞–Ω–∞:          {ba_median_cluster:.4f}")
        print(f"    F1-score:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π: {f1_weighted_cluster:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {f1_simple_cluster:.4f}")
        print(f"      –ú–µ–¥–∏–∞–Ω–∞:          {f1_median_cluster:.4f}")

        print(f"\n  –° –û–ü–¢–ò–ú–ê–õ–¨–ù–´–ú–ò –ü–û–†–û–ì–ê–ú–ò:")
        print(f"    Balanced Accuracy:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_weighted_optimal:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_simple_optimal:.4f}")
        print(f"      –ú–µ–¥–∏–∞–Ω–∞:          {ba_median_optimal:.4f}")
        print(f"    F1-score:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π: {f1_weighted_optimal:.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {f1_simple_optimal:.4f}")
        print(f"      –ú–µ–¥–∏–∞–Ω–∞:          {f1_median_optimal:.4f}")

        # –£–ª—É—á—à–µ–Ω–∏–µ
        ba_improvement_weighted = ba_weighted_optimal - ba_weighted_cluster
        ba_improvement_simple = ba_simple_optimal - ba_simple_cluster

        print(f"\n  –£–õ–£–ß–®–ï–ù–ò–ï (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ vs –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –ø–æ—Ä–æ–≥–∏):")
        print(f"    Balanced Accuracy:")
        print(f"      –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è: {ba_improvement_weighted:+.4f}")
        print(f"      –ü—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ:  {ba_improvement_simple:+.4f}")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_sizes = [r['size'] for r in cluster_results.values()]
        print(f"\n  –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
        print(f"    –ú–∏–Ω–∏–º—É–º:    {min(cluster_sizes)}")
        print(f"    –ú–∞–∫—Å–∏–º—É–º:   {max(cluster_sizes)}")
        print(f"    –ú–µ–¥–∏–∞–Ω–∞:    {np.median(cluster_sizes):.0f}")
        print(f"    75-–π –ø–µ—Ä—Ü.: {np.percentile(cluster_sizes, 75):.0f}")

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Å –∫–∞—á–µ—Å—Ç–≤–æ–º
        sizes = np.array(cluster_sizes)
        ba_cluster_vals = np.array([r['ba_cluster_thresh'] for r in cluster_results.values()])

        if len(sizes) > 1:
            correlation = np.corrcoef(sizes, ba_cluster_vals)[0, 1]
            print(f"    –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Ä–∞–∑–º–µ—Ä-–∫–∞—á–µ—Å—Ç–≤–æ: {correlation:.3f}")

    # ==================== –°–†–ê–í–ù–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô VS –ö–õ–ê–°–¢–ï–†–û–í ====================
    if category_results and cluster_results:
        print(f'\n{"=" * 60}')
        print("–°–†–ê–í–ù–ï–ù–ò–ï –ö–ê–¢–ï–ì–û–†–ò–ô –ò –ö–õ–ê–°–¢–ï–†–û–í")
        print(f'{"=" * 60}')

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã—Ö BA
        print(f"\n‚öñÔ∏è  –°–†–ê–í–ù–ï–ù–ò–ï –ü–û Balanced Accuracy:")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (—Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è):     {ba_weighted_cat:.4f}")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ã (—Å –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏): {ba_weighted_cluster:.4f}")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ã (—Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏): {ba_weighted_optimal:.4f}")

        improvement_vs_cat_cluster = ba_weighted_cluster - ba_weighted_cat
        improvement_vs_cat_optimal = ba_weighted_optimal - ba_weighted_cat

        print(f"\nüìà –£–õ–£–ß–®–ï–ù–ò–ï –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–û –ö–ê–¢–ï–ì–û–†–ò–ô:")
        print(f"  –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:   {improvement_vs_cat_cluster:+.4f}")
        print(f"  –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:  {improvement_vs_cat_optimal:+.4f}")

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–æ–≤
        print(f"\nüìä –ê–ù–ê–õ–ò–ó –†–ê–ó–ú–ï–†–û–í –ì–†–£–ü–ü:")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {len(category_results)} –≥—Ä—É–ø–ø")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ã:  {len(cluster_results)} –≥—Ä—É–ø–ø")

        # –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –¥–∞–Ω–Ω—ã—Ö
        cat_coverage = sum(r['size'] for r in category_results.values()) / len(feat) * 100
        cluster_coverage = sum(r['size'] for r in cluster_results.values()) / len(feat) * 100

        print(f"\nüìà –ü–û–ö–†–´–¢–ò–ï –î–ê–ù–ù–´–•:")
        print(f"  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ–∫—Ä—ã–≤–∞—é—Ç: {cat_coverage:.1f}% –¥–∞–Ω–Ω—ã—Ö")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä—ã –ø–æ–∫—Ä—ã–≤–∞—é—Ç:  {cluster_coverage:.1f}% –¥–∞–Ω–Ω—ã—Ö")

    # ==================== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–†–£–ü–ù–´–• –ì–†–£–ü–ü ====================
    if cluster_results:
        print(f'\n{"=" * 60}')
        print("–ê–ù–ê–õ–ò–ó –ö–†–£–ü–ù–ï–ô–®–ò–• –ö–õ–ê–°–¢–ï–†–û–í (top-10 –ø–æ —Ä–∞–∑–º–µ—Ä—É)")
        print(f'{"=" * 60}')

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É
        sorted_clusters = sorted(cluster_results.items(),
                                 key=lambda x: x[1]['size'],
                                 reverse=True)[:10]

        print(f"\n{'–ö–ª–∞—Å—Ç–µ—Ä':<30} {'–†–∞–∑–º–µ—Ä':>8} {'–í–µ—Å,%':>7} {'BA_–∫–ª–∞—Å—Ç':>8} {'BA_–æ–ø—Ç–∏–º':>8} {'ŒîBA':>6}")
        print("-" * 75)

        total_size_top10 = 0
        total_weight_top10 = 0

        for cluster_name, metrics in sorted_clusters:
            size = metrics['size']
            weight_pct = metrics['weight'] * 100
            ba_cluster = metrics['ba_cluster_thresh']
            ba_optimal = metrics['ba_optimal_thresh']
            delta_ba = ba_optimal - ba_cluster

            total_size_top10 += size
            total_weight_top10 += metrics['weight']

            print(f"{cluster_name:<30} {size:>8} {weight_pct:>6.1f}% "
                  f"{ba_cluster:>8.3f} {ba_optimal:>8.3f} {delta_ba:>+6.3f}")

        print("-" * 75)
        print(f"–ò—Ç–æ–≥–æ —Ç–æ–ø-10: {total_size_top10} –æ–±—ä–µ–∫—Ç–æ–≤ "
              f"({total_weight_top10 * 100:.1f}% –¥–∞–Ω–Ω—ã—Ö)")

        # –°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è BA –¥–ª—è —Ç–æ–ø-10
        ba_weighted_top10_cluster = sum(
            m['ba_cluster_thresh'] * m['weight'] / total_weight_top10
            for _, m in sorted_clusters
        )
        ba_weighted_top10_optimal = sum(
            m['ba_optimal_thresh'] * m['weight'] / total_weight_top10
            for _, m in sorted_clusters
        )

        print(f"–°—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è BA —Ç–æ–ø-10:")
        print(f"  –° –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏:  {ba_weighted_top10_cluster:.4f}")
        print(f"  –° –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏: {ba_weighted_top10_optimal:.4f}")

    return {
        'category_results': category_results,
        'cluster_results': cluster_results,
        'ba_weighted_cat': ba_weighted_cat if category_results else None,
        'ba_weighted_cluster': ba_weighted_cluster if cluster_results else None,
        'ba_weighted_optimal': ba_weighted_optimal if cluster_results else None,
        'improvement_vs_cat_cluster': improvement_vs_cat_cluster if (category_results and cluster_results) else None,
        'improvement_vs_cat_optimal': improvement_vs_cat_optimal if (category_results and cluster_results) else None
    }



def test_model(model, X_train, y_train, test_data_list, cat_features):
    print(cat_features)
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ —Ç—Ä–µ–π–Ω–µ
    optimizer = LeafClusterThresholdOptimizer(
        model=model,
        n_trees=2,
        min_cluster_size=100
    )

    feature_optimizer = FeatureClusterThresholdOptimizer(
        n_clusters=30,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        min_cluster_size=50,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
        use_pca=True,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PCA
        pca_components=20,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç PCA
        random_state=42
    )

    mixed_optimizer = MixedDataClusterOptimizer(
        n_clusters=20,
        min_cluster_size=50,
        categorical_distance='jaccard',  # –î–ª—è one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        metric='balanced_accuracy',
        cat_features=cat_features,
        random_state=42
    )


    # –û–±—É—á–∞–µ–º –Ω–∞ —Ç—Ä–µ–π–Ω–µ
    y_pred_proba_train = model.predict_proba(X_train)[:, 1]
    #optimizer.fit(X_train, y_train, y_pred_proba_train)
    mixed_optimizer.fit(X_train, y_train, y_pred_proba_train)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏ (—Å—Ä–µ–¥–Ω–µ–≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
    results = test_with_clusters_weighted_mixed(
        _model=model,
        _test_data=test_data_list,
        _cat_features=cat_features,
        feature_optimizer=mixed_optimizer,
        _inference=False
    )

    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if results['improvement_vs_cat_optimal']:
        print(f"\n{'=' * 60}")
        print("–ò–¢–û–ì–û–í–û–ï –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï")
        print(f"{'=' * 60}")

        if results['improvement_vs_cat_optimal'] > 0.01:
            print("‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –°–£–©–ï–°–¢–í–ï–ù–ù–û —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ!")
        elif results['improvement_vs_cat_optimal'] > 0:
            print("‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ù–ï–ú–ù–û–ì–û —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ")
        elif results['improvement_vs_cat_optimal'] > -0.01:
            print("‚ö†Ô∏è  –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ù–ï –£–•–£–î–®–ê–ï–¢ –∫–∞—á–µ—Å—Ç–≤–æ")
        else:
            print("‚ùå –ö–ª–∞—Å—Ç–µ—Ä–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –£–•–£–î–®–ê–ï–¢ –∫–∞—á–µ—Å—Ç–≤–æ")

        print(
            f"\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {'–ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨' if results['improvement_vs_cat_optimal'] > 0 else '–ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å'} –∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ –ø–æ—Ä–æ–≥–∏")